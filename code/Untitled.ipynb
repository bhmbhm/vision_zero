{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"/Users/jmocko/Desktop/vision_zero/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmocko/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (27,77,99,107) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "int_coords = pd.read_csv(wd+\"intersections_coords.csv\")\n",
    "crash_int_cw = pd.read_csv(wd+\"crashes_intersections_crosswalk.csv\")\n",
    "int_trans = pd.read_csv(wd+\"intersections_transit.csv\")\n",
    "crash_df = pd.read_csv(wd+\"txdot_cris_crashes_harris_fortbend_montgomery_2014_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating merged dataframe\n",
    "full_df = pd.merge(int_coords,\n",
    "        int_trans,\n",
    "        how = \"left\",\n",
    "        on = ['Intersection.ID'])\n",
    "\n",
    "full_df = pd.merge(crash_int_cw,\n",
    "        full_df,\n",
    "        how = \"left\",\n",
    "        on = ['Intersection.ID'])\n",
    "\n",
    "full_df = pd.merge(crash_df,\n",
    "        full_df,\n",
    "        how = \"left\",\n",
    "        on = ['Crash.ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of columns to keep for downstream analysis, as determined by Lauren and Kelsey\n",
    "\n",
    "keep_cols = [\n",
    "        'Intersection.ID',\n",
    "        'Crash.ID',\n",
    "#     'Street.Name',\n",
    "#     'Intersecting.Street.Name',\n",
    "    'County', #filter for Harris, Montgomery, Fort Bend\n",
    "    'Intersection.Related', # filter for intersection or intersection related\n",
    "    'Speed.Limit',\n",
    "    'Crash.Severity',\n",
    "    'Number.of.Lanes',\n",
    "    'Number.of.Entering.Roads',\n",
    "    'Traffic.Control.Type',\n",
    "#     'Road.Class',\n",
    "    'Roadbed.Width',\n",
    "    'Roadway.Alignment',\n",
    "    'Roadway.Function',\n",
    "    'Roadway.Relation',\n",
    "    'Roadway.Part',\n",
    "    'Roadway.Type',\n",
    "    'Light.Condition',\n",
    "    'First.Harmful.Event',\n",
    "    'n_bikes',\n",
    "    'n_cars',\n",
    "    'n_peds',\n",
    "#     'n_train',\n",
    "    'Median.Width',\n",
    "    'n_roads',\n",
    "    'n_street_names', \n",
    "    'n_transit_routes_200ft',\n",
    "    'n_transit_routes_400ft', \n",
    "    'n_transit_routes_closest',\n",
    "    'n_transit_stops_200ft', \n",
    "    'n_transit_stops_400ft',\n",
    "    'n_transit_stops_closest', \n",
    "    'n_transit_trips_200ft',\n",
    "    'n_transit_trips_400ft', \n",
    "    'n_transit_trips_closest', \n",
    "    'period',\n",
    "    'road_classes', \n",
    "    'street_names', \n",
    "    'x_x', \n",
    "    'x_y', \n",
    "    'y_x', \n",
    "    'y_y']\n",
    "\n",
    "limit_df = full_df.loc[:,keep_cols]\n",
    "limit_df = limit_df.rename(index = str,\n",
    "                          columns = {'x_x':'x_intersection',\n",
    "                                    'x_y':'x_crash',\n",
    "                                    'y_x':'y_intersection',\n",
    "                                    'y_y':'y_crash'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmocko/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "#Filter the dataframe by county (afterwards, don't need this column)\n",
    "limit_df = limit_df.loc[(limit_df['County'].str.contains('FORT BEND')) |\n",
    "                       (limit_df['County'].str.contains('HARRIS')) |\n",
    "                       (limit_df['County'].str.contains('MONTGOMERY'))]\n",
    "\n",
    "#Filter the dataframe to only have intersection and intersection related rows (afterwards, don't need this column)\n",
    "limit_df = limit_df.loc[((limit_df['Intersection.Related'].str.contains('INTERSECTION')) |\n",
    "                       (limit_df['Intersection.Related'].str.contains('INTERSECTION RELATED'))) &\n",
    "                       ~(limit_df['Intersection.Related'].str.contains('NON INTERSECTION'))]\n",
    "\n",
    "#Data Cleaning\n",
    "#SpeedLimit - turn all negatives into NaN\n",
    "#Round all numbers with %5 != 0\n",
    "limit_df.loc[limit_df['Speed.Limit']<0, 'Speed.Limit'] = np.nan\n",
    "limit_df.loc[limit_df['Speed.Limit']%5 != 0, 'Speed.Limit'] = round(limit_df.loc[limit_df['Speed.Limit']%5 != 0, 'Speed.Limit'],-1)\n",
    "\n",
    "#Clean Severity\n",
    "limit_df['Crash.Severity'] = limit_df['Crash.Severity'].str.upper()\n",
    "limit_df.loc[limit_df['Crash.Severity']=='N - NOT INJURED', 'Crash.Severity'] = \"NOT INJURED\"\n",
    "limit_df.loc[limit_df['Crash.Severity']=='C - POSSIBLE INJURY', 'Crash.Severity'] = \"POSSIBLE INJURY\"\n",
    "limit_df.loc[limit_df['Crash.Severity']=='99 - UNKNOWN', 'Crash.Severity'] = \"UNKNOWN\"\n",
    "limit_df.loc[limit_df['Crash.Severity']=='B - NON-INCAPACITATING INJURY', 'Crash.Severity'] = \"NON-INCAPACITATING INJURY\"\n",
    "limit_df.loc[limit_df['Crash.Severity']=='A - SUSPECTED SERIOUS INJURY', 'Crash.Severity'] = \"SUSPECTED SERIOUS INJURY\"\n",
    "limit_df.loc[limit_df['Crash.Severity']=='K - KILLED', 'Crash.Severity'] = \"KILLED\"\n",
    "\n",
    "#Create a binary for CrashSeverity based on injured vs non-injured, and excluding all unknown\n",
    "limit_df['Crash.Severity_Binary'] = \"INJURED\"\n",
    "limit_df.loc[(limit_df['Crash.Severity']=='NOT INJURED'), 'Crash.Severity_Binary'] = \"NON-INJURED\"\n",
    "limit_df = limit_df.loc[~(limit_df['Crash.Severity']==\"UNKNOWN\")]\n",
    "\n",
    "#RoadbedWidth - all No Data and nan to NaN\n",
    "limit_df.loc[limit_df['Roadbed.Width']=='No Data', 'Roadbed.Width'] = np.nan\n",
    "limit_df.loc[limit_df['Roadbed.Width']=='nan', 'Roadbed.Width'] = np.nan\n",
    "limit_df['Roadbed.Width'] = limit_df['Roadbed.Width'].astype(float)\n",
    "\n",
    "#NumberofLanes - all No Data and nan to NaN\n",
    "limit_df.loc[limit_df['Number.of.Lanes']=='No Data', 'Number.of.Lanes'] = np.nan\n",
    "limit_df.loc[limit_df['Number.of.Lanes']=='nan', 'Number.of.Lanes'] = np.nan\n",
    "limit_df['Number.of.Lanes'] = limit_df['Number.of.Lanes'].astype(float)\n",
    "\n",
    "#RoadwayType - all No Data and nan to NaN\n",
    "limit_df.loc[limit_df['Roadway.Type']=='No Data', 'Roadway.Type'] = np.nan\n",
    "limit_df.loc[limit_df['Roadway.Type']=='nan', 'Roadway.Type'] = np.nan\n",
    "\n",
    "#MedianWidth - all No Data and nan to NaN\n",
    "limit_df.loc[limit_df['Median.Width']=='No Data', 'Median.Width'] = np.nan\n",
    "limit_df.loc[limit_df['Median.Width']=='nan', 'Median.Width'] = np.nan\n",
    "limit_df['Median.Width'] = limit_df['Median.Width'].astype(float)\n",
    "\n",
    "#Get the number from the Number.of.Entering.Roads\n",
    "def get_first_num(row):\n",
    "    return row.split(\" - \")[0]\n",
    "\n",
    "limit_df['num_entering_roads'] = limit_df['Number.of.Entering.Roads'].apply(lambda x: get_first_num(x))\n",
    "limit_df['num_entering_roads'] = limit_df['num_entering_roads'].replace(\"97\", np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Intersection.ID', 'Crash.ID', 'County', 'Intersection.Related',\n",
       "       'Speed.Limit', 'Crash.Severity', 'Number.of.Lanes',\n",
       "       'Number.of.Entering.Roads', 'Traffic.Control.Type', 'Road.Class',\n",
       "       'Roadbed.Width', 'Roadway.Alignment', 'Roadway.Function',\n",
       "       'Roadway.Relation', 'Roadway.Part', 'Roadway.Type', 'Light.Condition',\n",
       "       'First.Harmful.Event', 'n_bikes', 'n_cars', 'n_peds', 'Median.Width',\n",
       "       'n_roads', 'n_street_names', 'n_transit_routes_200ft',\n",
       "       'n_transit_routes_400ft', 'n_transit_routes_closest',\n",
       "       'n_transit_stops_200ft', 'n_transit_stops_400ft',\n",
       "       'n_transit_stops_closest', 'n_transit_trips_200ft',\n",
       "       'n_transit_trips_400ft', 'n_transit_trips_closest', 'period',\n",
       "       'road_classes', 'street_names', 'x_intersection', 'x_crash',\n",
       "       'y_intersection', 'y_crash', 'Crash.Severity_Binary',\n",
       "       'num_entering_roads'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe that has the categorical variables now all as dummy variables\n",
    "crash_df = limit_df.loc[:,['Intersection.ID',\n",
    "                                  'Crash.ID',\n",
    "                                  'road_classes', \n",
    "                                  'street_names',\n",
    "                                  'Speed.Limit',\n",
    "                                  'Roadbed.Width', \n",
    "                                  'Number.of.Lanes',\n",
    "                                  'Median.Width',\n",
    "                                  'n_bikes', \n",
    "                                  'n_cars',\n",
    "                                  'n_peds',\n",
    "                                  'n_roads', \n",
    "                                  'n_street_names', \n",
    "                                  'n_transit_routes_200ft',\n",
    "                                  'n_transit_routes_400ft', \n",
    "                                  'n_transit_routes_closest',\n",
    "                                  'n_transit_stops_200ft', \n",
    "                                  'n_transit_stops_400ft',\n",
    "                                  'n_transit_stops_closest', \n",
    "                                  'n_transit_trips_200ft',\n",
    "                                  'n_transit_trips_400ft', \n",
    "                                  'n_transit_trips_closest']]\n",
    "\n",
    "def create_dummies(df, col):\n",
    "    dummies = pd.get_dummies(df[col])\n",
    "    dummies.columns = [col+\"_\"+column for column in dummies.columns.values]\n",
    "    return dummies\n",
    "\n",
    "cols_for_dummies = ['Crash.Severity',\n",
    "                    'Number.of.Entering.Roads', \n",
    "                    'Traffic.Control.Type',\n",
    "                    'Roadway.Alignment', \n",
    "                    'Roadway.Function',\n",
    "                    'Roadway.Relation', \n",
    "                    'Roadway.Part', \n",
    "                    'Roadway.Type', \n",
    "                    'Light.Condition',\n",
    "                    'Crash.Severity_Binary']\n",
    "\n",
    "for col in cols_for_dummies:\n",
    "    crash_df = pd.concat([crash_df,\n",
    "                               create_dummies(limit_df, col)],\n",
    "                                axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column for the number of crashes per intersection\n",
    "crash_num = crash_df.loc[:,['Intersection.ID',\n",
    "            'Crash.ID']].groupby(['Intersection.ID']).count().reset_index()\n",
    "crash_num = crash_num.rename(index = str, columns = {'Crash.ID':'crash_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for the aggregated crash data by intersection\n",
    "#Probably should have named this variable intersection_df, but oh well\n",
    "intersection_df = crash_df.loc[:,['Intersection.ID',\n",
    "                                 'road_classes',\n",
    "                                 'street_names']].drop_duplicates()\n",
    "intersection_df = pd.merge(intersection_df, crash_num, how = \"left\", on = \"Intersection.ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to aggregate the continuous and categorical variable columns\n",
    "#Continuous - get min, max, mean, and sum (not all are relevant to all variables)\n",
    "#Categorical - get mean (which is essentially a proportion, good for modeling)\n",
    "\n",
    "def continuous_agg(df, agg_col):\n",
    "    grouped_df = df.loc[:,['Intersection.ID',\n",
    "            agg_col]].groupby(['Intersection.ID']).agg([np.min, np.max, np.mean])\n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "    grouped_df = grouped_df.reset_index()\n",
    "    return grouped_df\n",
    "\n",
    "def categorical_agg(df, agg_col):\n",
    "    grouped_df = df.loc[:,['Intersection.ID',\n",
    "            agg_col]].groupby(['Intersection.ID']).agg([np.mean])\n",
    "    grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "    grouped_df = grouped_df.reset_index()\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = ['Intersection.ID', 'road_classes', 'street_names', 'crash_num']\n",
    "continuous_cols = ['Speed.Limit', \n",
    "                    'Roadbed.Width', \n",
    "                    'Number.of.Lanes', \n",
    "                    'Median.Width']\n",
    "\n",
    "#Run through the continuous variables and run the continuous aggregator\n",
    "for col in continuous_cols:\n",
    "#     print(f\"Now doing {col}\")\n",
    "    agged_df = continuous_agg(crash_df, col)\n",
    "    intersection_df = pd.merge(intersection_df, agged_df, how = \"left\", on = \"Intersection.ID\")\n",
    "\n",
    "#Columns for categorical agg\n",
    "cat_cols = list(crash_df.columns)\n",
    "cat_cols = [col for col in cat_cols if col != 'Crash.ID']\n",
    "cat_cols = [col for col in cat_cols if col not in base_cols]\n",
    "cat_cols = [col for col in cat_cols if col not in continuous_cols]\n",
    "\n",
    "#Run through the categorical variables and get the means    \n",
    "for col in cat_cols:\n",
    "#     print(f\"Now doing {col}\")\n",
    "    agged_df = categorical_agg(crash_df, col)\n",
    "    intersection_df = pd.merge(intersection_df, agged_df, how = \"left\", on = \"Intersection.ID\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating unbiased means\n",
    "intersection_df['Speed.Limit_unbiased_mean'] = (intersection_df['Speed.Limit_amin']+intersection_df['Speed.Limit_amax'])/2\n",
    "intersection_df['Number.of.Lanes_unbiased_mean'] = (intersection_df['Number.of.Lanes_amin']+intersection_df['Number.of.Lanes_amax'])/2\n",
    "\n",
    "#Renumbering the categorical variables to 0 or 1, with a threshold of 0.10\n",
    "categorical_cols = list(intersection_df.columns)[30:-4]\n",
    "for col in categorical_cols:\n",
    "    intersection_df.loc[intersection_df[col]>0.10, col] = 1\n",
    "    intersection_df.loc[intersection_df[col]<=0.10, col] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the new intersection crash aggregated dataset for modeling\n",
    "intersection_df.to_csv(wd+'full_aggregated_intersection_dataset.csv', index = False)\n",
    "\n",
    "intersection_df_to_model = intersection_df.drop(['Roadbed.Width_amin',\n",
    "                                             'Roadbed.Width_amax',\n",
    "                                             'Roadbed.Width_mean',\n",
    "                                             'Number.of.Lanes_amin',\n",
    "                                             'Number.of.Lanes_amax',\n",
    "                                             'Number.of.Lanes_mean',\n",
    "                                             'Median.Width_amin',\n",
    "                                             'Median.Width_amax',\n",
    "                                             'Median.Width_mean']\n",
    "                                             , axis = 1)\n",
    "\n",
    "#Save the new intersection crash aggregated for modeling\n",
    "intersection_df_to_model.to_csv(wd+'intersection_dataset_to_model.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
